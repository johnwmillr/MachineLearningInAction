{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow the naïve Bayes examples from chapter 4 of Harrington's *Machine Learning in Action* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5.1** *Prepare: making word vectors from text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=['my dog has flea problems help please',\n",
    "                 'maybe not take him to dog park stupid',\n",
    "                 'my dalmation is so cute I love him',                 \n",
    "                 'you should stop posting stupid worthless garbage',\n",
    "                 'mr licks ate my steak how do I stop him',\n",
    "                 'quit buying worthless dog food stupid']\n",
    "    classVector = [0,1,0,1,0,1] # Class 1 is abusive, 0 is not\n",
    "    return [doc.split() for doc in postingList], classVector\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for doc in dataSet:\n",
    "        vocabSet = vocabSet |\\\n",
    "        set(d.lower() for d in doc) # Create union of two sets (unique values only)\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # Converts a list of words into a vector.\n",
    "    \n",
    "    # Each unique word (within our vocabulary) is a feature. So if we\n",
    "    # have N words in our vocabulary, the input sentence can be represented\n",
    "    # as a point in N-dimensional space. Sentences that use a similar\n",
    "    # vocabulary will cluster in this \"vocab space\"\n",
    "    \n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        word = word.lower()\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"The word: %s is not in my vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    n_trainDocs = len(trainMatrix)\n",
    "    n_words = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(n_trainDocs)\n",
    "    p0Num = ones(n_words); p1Num = ones(n_words)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    for i in range(n_trainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "def classifyNB(vecToClassify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vecToClassify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vecToClassify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "def testingNB(testEntry):\n",
    "    posts, classes = loadDataSet()        \n",
    "    myVocabList = createVocabList(posts)\n",
    "\n",
    "    # Create a training matrix by converting the forum post words to vectors\n",
    "    trainMat = []\n",
    "    for postInDoc in posts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postInDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat), array(classes))\n",
    "    \n",
    "    # Classify the test value\n",
    "    classNames = ['Supportive','Abusive']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry.split()))\n",
    "    print(testEntry, 'classified as: ',\\\n",
    "          classNames[classifyNB(thisDoc,p0V,p1V,pAb)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love my dalmation', 'classified as: ', 'Supportive')\n",
      "('Dog is stupid garbage', 'classified as: ', 'Abusive')\n"
     ]
    }
   ],
   "source": [
    "testingNB('I love my dalmation')\n",
    "testingNB('Dog is stupid garbage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6** *Example: classifying spam email with naïve Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    # Keep track of multiple occurences of words\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text from the emails provided\n",
    "emailText = open('./SourceMaterial/Ch04/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText = []\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('./SourceMaterial/Ch04/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('./SourceMaterial/Ch04/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50); testSet = []\n",
    "    \n",
    "    # Pick 10 random indices for the test set\n",
    "    for i in range(10):\n",
    "        randIdx = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIdx])\n",
    "        del(trainingSet[randIdx])\n",
    "    \n",
    "    # Build the training set (after test words have been removed)\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIdx in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIdx]))\n",
    "        trainClasses.append(classList[docIdx])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    \n",
    "    # Evaluate classifier on the test set\n",
    "    errorCount = 0\n",
    "    for docIdx in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIdx])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIdx]:\n",
    "            errorCount += 1\n",
    "\n",
    "    errorRate = float(errorCount)/len(testSet)\n",
    "#     print(\"The error rate is: \", errorRate)\n",
    "    return errorRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error rate is: 7.60 percent.\n"
     ]
    }
   ],
   "source": [
    "# The spamTest() function performs hold-out cross validation\n",
    "# To get a realistic sense of the error rate, we need to average\n",
    "# multiple tests\n",
    "\n",
    "n_runs = 100\n",
    "totalError = 0.0\n",
    "for n in range(n_runs):\n",
    "    totalError = totalError + spamTest()\n",
    "    \n",
    "print(\"The average error rate is: %.02f percent.\" % (100*totalError/float(n_runs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:**\n",
    "\n",
    "I should compare the performance (accuracy and efficiency) of the Naive Bayes classifier to the decision tree from chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
